\documentclass{article}
\usepackage[final]{corl_2020} % Use this for the initial submission.
\usepackage[utf8]{inputenc}

% \usepackage[numbers]{natbib}
% \usepackage[bookmarks=true]{hyperref}

\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsthm}

\usepackage{algorithm, algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{todonotes}

\usepackage{subcaption}
\usepackage{booktabs}

\title{Using Disentangled Metrics of Image deformation for Improved Grasp Candidate Transfer on Novel Objects}
\date{\today}
\author{Cian Eastwood, Chris Williams, Craig Innes, Yordan Hristov, Subramanian Ramamoorthy}

\begin{document}

\maketitle

% \begin{abstract}
%     Insert Abstract Here
% \end{abstract}

\section{Introduction / Idea}

% difference, we have a much better shot at learning how to adapt existing grasps to new target objects (for instance, if a large rigid transformation is needed, but no deformation, then perhaps our target object is largely similar to the archetypal one, but just in a different pose. If the textural difference is large, perhaps only the contact friction coefficient needs changing in assessing good grasps).

Recent work on disentangled image metrics \cite{cian2021} allow us to disentangle the ``distance'' between a source/target image into a rigid-transformation, a deformation and a textural difference. This ability to separate image difference into \emph{separate parts} is potentially useful for the robotic task of finding good grasps for objects.

There are many approaches to finding good grasps. We classify approaches by the amount of knowledge they assume about the target object \cite{Bohg2014}. On one extreme, we have a 3-D model of the target object, information on its physical properties, and existing analytical data on good grasp positions. The problem reduces to object pose estimation and filtering a grasp candidates based on reachability. On the other extreme, the object is completely novel --- We know only what we can gather via the robot's sensors (e.g., we use local image features to predict good grasps).

Between these two extremes, we might encounter a novel object, but it is \emph{familiar} to us. This is the scenario we are interested in. Typically, this means we already have a database of good grasps for a set of archetypal objects, and the object we are currently looking at may be a variation on this. Here, the problem is: How do we match our target object to the ``closest'' archetypal object, how do we quantify how the two differ (e.g., in terms of shape deformation, texture, physical properties...), and how do we use this information about their difference to adapt known good grasps on our archetypal object to our target object?

Many approaches exist to learn this correspondence between known and target object grasps. Some assume access to 3D models of both objects\cite{Hillenbrand2012}, or the full point clouds \cite{Brandi2015}. Others additionally incorporate demonstrated trajectories and colour masking to deal with finding candidate grasps in clutter \cite{Lee2015}. Many allow for partially observed point clouds by matching shape affordances \cite{Liu2019}, leveraging object category info \cite{Rodriguez2018}, or focusing on salient 3D key-points \cite{Manuelli2019}. Alternatively, some approaches instead opt to learn a lower-dimensional latent space using e.g., Convolutional Neural Networks, then predict grasp candidates based on distance between objects in this embedding \cite{Pavlichenko2018, Detry2012}.

However, what if, due to hardware or task constraints, you only have access to a limited amount of sensory data about the target object (e.g., a single RGB image). Is it still possible to learn useful information about grasp correspondences? Images indeed contain lots of useful information about differences between an archetypal and target object (e.g., shape, pose, scene-context, texture as a proxy for contact-friction etc). The problem is that all these differences are entangled together in a single source of data (the image). 


% \todo[inline]{Possible line of criticism: Edge detection algorithms exist, background subtraction algorithms exist, image/shape decomposition algorithms exist. Wouldn't it be possible to actually disentangle / extract much of the information we require using a fairly standard ``old-fashioned'' computer vision pipeline?}

% \todo[inline]{Alternative Motivation Strategy: At the moment, the motivation is setup as "here is an important problem in robotics - perhaps we can pull in interesting image disentanglement techniques to help". Perhaps an ``easier'' sell is to start the motivation from Cian's end: "Here is a novel approach in the field of image disentanglement --- we can apply it to problems in robotics to see how it might be used as a replacement for existing techniques, and to provide some insight and analysis as to what is going on (without any claims to state of the art grasping performance)}


\section{The Task / Method}

Given:

\paragraph{A Database of objects} $\mathcal{O} = \{O_1, \dots O_n\}$, $O_i = \langle c_i, G_i, \mathit{I}_i,  \textsc{urdf}_i \rangle$, where $c_i$ is object category, $G_i$ is a list of good grasp candidate positions, $I_i$ is an RGB image/images of object, and $\textsc{urdf}_i$ is a collection (urdf-style?) information about the 3d model of the object, its physical properties etc.

\paragraph{Target}: $I_{target}$ an RGB image of the object

\paragraph{Goals / Sub-goals}: Find $G_{target}$ by the following steps:

\begin{enumerate}
    \item Find a Disentangled mapping between best $I_i$ and $I_{target}$
    \item Use disentangled mapping to learn correspondence between grasp points $G_i$ and potential good grasp points $G_{target}$
    \item Evaluate how well warping works
\end{enumerate}

% \subsection{Alternative Applications}

% At the moment, this problem is phrased as a standard ``Find good grasps'' problem. However, there are a couple of potential, more recent tasks we may want to incorporate:

% One is on finding grasps that ``afford'' particular tasks/uses, or that work under functional task constraints (e.g., a robot-human handover task where a human with limited mobility may not find particular handover poses comfortable/possible) \cite{ardon2020affordances}, \cite{Ardon2020b}

% Another is \emph{pre-grasp manipulation}. It may be that in the current scenario, there are \emph{no} ways to grasp the target object without first moving it into a graspable position (e.g., by sliding a thin book the the edge of the table first) \cite{King2016}. Can such pregrasp manipulation tasks be transferred from archetypes in a similar way?

\section{Experiment / Evaluation}

\subsection{Phase 1: All Sim}

Both the database of objects (with known grasp positions) and target object, are simulated 3D models. Images are generated by rendering the 3D models in scenes. We can rapidly try out many candidate grasps, and assess whether our learned disentangled model produces good candidate grasps by running the target model in simulation and gathering any required data on contact forces, final position etc rapidly and automatically.

\subsection{Phase 2: Real Objects}

Once we are confident that system is producing decent results, swap out simulated target object for real physical objects in a real robot environment.

\subsection{Evaluation Questions}

\begin{itemize}
    \item  What are our benchmark comparisons? What is the scope? \emph{only} image-based methods? Point-cloud deformation approaches? Onces which have heavy amounts of local feature engineering? There is also much ``big-data'' / ``deep-learning'' approaches which play in this space (e.g., Dex-Net \cite{mahler2017dex}). What about RL-based / LfD-style approaches?
    \item What are our actual metrics of success? Simply "Was this grasp successful on the robot / in sim?", What about success from the image-comparison side? There are also potentially grasp quality metrics that we may want to compare with (e.g., does our image-deformation-to-grasp-adaptation mapping correlate with metrics of quality grasps \cite{roa2015grasp}? 
\end{itemize}


\bibliography{references}

\end{document}
